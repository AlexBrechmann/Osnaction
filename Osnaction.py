#!/usr/bin/env python
# coding: utf-8

# In[5]:


import numpy as np
import pandas as pd


# In[15]:


data.to_csv('Datensatz_events.csv')


# In[6]:


# Daten einlesen
data = pd.read_excel('Datensatz_Osnaaction.xlsx')


# In[7]:


data.head()


# ## Filter

# TODO: Fragen durch Terminal beantworten

# In[11]:


# Antwort des Nutzers 
val0 = 1  # Indoor
val1 = 0 # Preis
val21 = 1  # Familie
val22 = 1  # Paar
val31 = 0  # Abenteuer
val32 = 1  # Erholung

# TODO: data_f und data vergleichen, und dann Index der gleichen Elemente speichern
# logical masking for questions
data_f = data[data['Indoor ']==val0]
data_f = data_f[data_f['Preis']==val1]
data_f = data_f[data_f['Familie']==val21]
data_f = data_f[data_f['Paar']==val22]
data_f = data_f[data_f['Abenteuer']==val31]
data_f = data_f[data_f['Erholung']==val32]

data_f.head()


# ## Classify with all historys
# - Each user has a click history vector. The current user will get a recommendation based on recommendation other users with similar history vector have received. 
# - we need a distance measure between vectors with different length (each history vector has a different length) which is hard to define
# - Alternatively, we can use principal component analysis to reduce dimensionality
# - however when we have a big discrepancy between the length of the longest history vector and the length of the shortest history vector, lots of information will be lost

# ## Classify individual user
# - Take only history of current user
# - $recommend = argmax_j \frac{1}{I} \sum_{i \in I} Similarity(H_i, A_j)$ with $H_i$ being an i-th event in user history and $A_j$ being an j-th event in all events.
# - Similarity is simply defined as number of co-ocurrences as all features have binary values for simplicity.
# - In theory, one can use e.g. natural language processing to compute similarity between descrptions of two events

# In[125]:


def similarity_binary(x,y):
    '''
    x, pandas Series: an event
    y: ""
    '''
    # take only features with binary values
    x = x.filter(['Indoor ', 'Preis', 'Familie','Paar', 'Abenteuer', 'Erholung']).to_numpy()
    y = y.filter(['Indoor ', 'Preis', 'Familie','Paar', 'Abenteuer', 'Erholung']).to_numpy()
    return(np.sum(x == y))


def recommend_one_event(data, events_ID, history_ID):
    """
    compute avg. similarity and recommend a maximially similar event
    """
    maxsim = 0  # init with a very low similarity
    argmax_index = []  # no event found at the start
    for j in events_ID:  # iterate over all events
        # compute avg. similarity between j and events in history
        sumsim = 0
        for i in history_ID:
            sumsim += similarity_binary(data.iloc[i], data.iloc[j])
        sumsim = sumsim / len(history_ID)
        # take argmax
        if sumsim == maxsim:  # multiple maxima
            argmax_index.append(j)
        elif sumsim > maxsim:
            maxsim = sumsim
            argmax_index.clear()  # all maxima found until now are not maxima anymore
            argmax_index.append(j)
        else:
            pass
    return(argmax_index)


def recommend(data, events_ID, history_ID):
    # output ordered lists of events with its score


# In[134]:


recommend(data, range(30), [0,2,10])  # the user already booked ID=0,2,10


# ## Recursive neural network

# ## Softmax regression

# In[36]:


import keras
from sklearn.model_selection import train_test_split


# In[53]:


# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kBDtzH19SYfRbyCs1cjwmCnC-FE64_zp
"""

import csv
import random

with open('dataset.csv', mode='w') as datafile:
    datafile_writer = csv.writer(datafile, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)

    row1 = ["Persnr"]
    for x in range(1,31):
        row1.append("Input"+str(x))
    for x in range(1,31):
        row1.append("Output"+str(x))

    datafile_writer.writerow(row1)

    list2 = []
    for x in range(1,1000):
        del list2[:]
        list2.append(x)
        for i in range (1,31):
            ri = random.randint(0, 1)
            list2.append(ri)
            ro = random.randint(1,30)
        for x in range(1,31):
            if ro == x:
                list2.append(1)
            else:
                list2.append(0)
        datafile_writer.writerow(list2)


# In[55]:


N = 30 # num. events
history_data = np.genfromtxt('dataset.csv', delimiter=',')
X,Y = history_data[1:,:N], history_data[1:,N+1:]
X_train, X_test, y_train, y_test = train_test_split(X,Y, train_size=0.7)


# In[80]:


# architecture
model = keras.Sequential([
    keras.layers.Dense(units=5, activation='sigmoid'),  # ReLu or tanh at x=0 0 is bad
    keras.layers.Dense(units=10, activation='sigmoid'),
    keras.layers.Dense(units=N, activation='softmax')  # output vector is a simplex
])
model.compile(optimizer='adadelta', 
              loss='categorical_crossentropy')  # one-hot output labels

# training
history = model.fit(X_train, y_train, epochs=10)

# testing
model.evaluate(X_test, y_test)

# save model and load for next use
model.save('NN_recommend')


# Nehmen wir jetzt an, dass ein Nutzer sich von der Algorithmus empfohlen lassen will.

# In[82]:


X_new =  np.array([np.random.randint(0, high=2, size=30)])  # user wants a recommendation

model = keras.models.load_model('NN_recommend')
predictions = model.predict(X_new)
print(predictions)  # probability for each event
print('sum of all output should be one: {}'.  # small numerical error is allowed
      format(np.sum(model.predict(X_new)))) 

# TODO: combine filter results with predictions from NN


# # TODO: interface filters NN

# In[ ]:




